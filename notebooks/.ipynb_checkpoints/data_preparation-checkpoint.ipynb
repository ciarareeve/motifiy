{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference genome loaded.\n",
      "ChIP-seq data loaded. Total entries: 57887\n",
      "Filtered out 5 entries with non-human chromosomes.\n",
      "Remaining entries after filtering: 57882\n",
      "Sequences extracted. Total sequences: 57882\n",
      "Processed data saved.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "# Load genome\n",
    "genome = Fasta('../data/raw/hg38.fa')\n",
    "print(\"Reference genome loaded.\")\n",
    "\n",
    "# Load ChIP-seq data\n",
    "chip_seq_data = pd.read_csv('../data/raw/chseq2.bed', sep='\\t', header=None)\n",
    "chip_seq_data.columns = ['chrom', 'start', 'end', 'name', 'score', 'strand', 'signalValue', 'pValue', 'qValue', 'peak']\n",
    "print(\"ChIP-seq data loaded. Total entries:\", len(chip_seq_data))\n",
    "\n",
    "# Filter out non-human chromosomes\n",
    "valid_chromosomes = set(genome.keys())\n",
    "initial_count = len(chip_seq_data)\n",
    "chip_seq_data = chip_seq_data[chip_seq_data['chrom'].isin(valid_chromosomes)]\n",
    "filtered_count = len(chip_seq_data)\n",
    "print(f\"Filtered out {initial_count - filtered_count} entries with non-human chromosomes.\")\n",
    "print(\"Remaining entries after filtering:\", filtered_count)\n",
    "\n",
    "# Extract sequences\n",
    "sequences = []\n",
    "for index, row in chip_seq_data.iterrows():\n",
    "    try:\n",
    "        sequence = genome[row['chrom']][row['start']:row['end']].seq\n",
    "        sequences.append(sequence)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error extracting sequence for row {index}: {e}\")\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "print(\"Sequences extracted. Total sequences:\", len(sequences))\n",
    "\n",
    "# Save processed data\n",
    "np.save('../data/processed/X.npy', sequences)\n",
    "np.save('../data/processed/y.npy', chip_seq_data['signalValue'].values)\n",
    "print(\"Processed data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bf509-7397-4aaa-bc61-e200a9698b01",
   "metadata": {},
   "source": [
    "\n",
    "1. **`data_preparation.ipynb`**\n",
    "\n",
    "   **Description**:\n",
    "   - This notebook prepares the data by loading the reference genome and ChIP-seq peak data, extracting sequences based on the peak coordinates, and saving the processed data as NumPy arrays.\n",
    "\n",
    "   **Input**:\n",
    "   - Reference genome file (`hg19.fa`)\n",
    "   - ChIP-seq peak file (`chip_seq_data.bed`)\n",
    "\n",
    "   **Output**:\n",
    "   - Processed sequences (`X.npy`)\n",
    "   - Signal values (`y.npy`)\n",
    "\n",
    "   **Checks**:\n",
    "   - Ensure that `X.npy` and `y.npy` are created in the `data/processed/` directory.\n",
    "   - Verify the shape and content of the arrays to ensure they match expectations.\n",
    "     ```python\n",
    "     # Load and check the processed data\n",
    "     X = np.load('../data/processed/X.npy')\n",
    "     y = np.load('../data/processed/y.npy')\n",
    "     \n",
    "     # Check shapes\n",
    "     print(X.shape)  # Expected shape: (number of peaks, sequence length)\n",
    "     print(y.shape)  # Expected shape: (number of peaks,)\n",
    "\n",
    "     # Check content\n",
    "     print(X[0])  # Print the first sequence\n",
    "     print(y[0])  # Print the first signal value\n",
    "     ```\n",
    "\n",
    "2. **`model_training.ipynb`**\n",
    "\n",
    "   **Description**:\n",
    "   - This notebook trains the BPNet model using the processed data. It splits the data into training and validation sets, creates the model, trains it, and saves the trained model and training history.\n",
    "\n",
    "   **Input**:\n",
    "   - Processed sequences (`X.npy`)\n",
    "   - Signal values (`y.npy`)\n",
    "\n",
    "   **Output**:\n",
    "   - Trained model (`bpnet_model.h5`)\n",
    "   - Training history (`training_history.npy`)\n",
    "\n",
    "   **Checks**:\n",
    "   - Ensure that `bpnet_model.h5` and `training_history.npy` are created in the `results/model/` directory.\n",
    "   - Verify the training history to check for convergence and potential overfitting.\n",
    "     ```python\n",
    "     # Load and check the training history\n",
    "     history = np.load('../results/model/training_history.npy', allow_pickle=True).item()\n",
    "     \n",
    "     # Plot training and validation loss\n",
    "     import matplotlib.pyplot as plt\n",
    "     plt.plot(history['loss'], label='Training Loss')\n",
    "     plt.plot(history['val_loss'], label='Validation Loss')\n",
    "     plt.legend()\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "3. **`deepLIFT_attribution.ipynb`**\n",
    "\n",
    "   **Description**:\n",
    "   - This notebook uses the trained BPNet model to calculate feature attributions using DeepLIFT. It saves the attributions as a NumPy array.\n",
    "\n",
    "   **Input**:\n",
    "   - Trained model (`bpnet_model.h5`)\n",
    "   - Validation data (`X_val.npy`)\n",
    "\n",
    "   **Output**:\n",
    "   - Attributions (`attributions.npy`)\n",
    "\n",
    "   **Checks**:\n",
    "   - Ensure that `attributions.npy` is created in the `results/attributions/` directory.\n",
    "   - Verify the content and shape of the attributions to ensure they are correctly calculated.\n",
    "     ```python\n",
    "     # Load and check the attributions\n",
    "     attributions = np.load('../results/attributions/attributions.npy')\n",
    "     \n",
    "     # Check shape\n",
    "     print(attributions.shape)  # Expected shape: (number of validation samples, sequence length)\n",
    "\n",
    "     # Check content\n",
    "     print(attributions[0])  # Print the first attribution\n",
    "     ```\n",
    "\n",
    "4. **`clustering.ipynb`**\n",
    "\n",
    "   **Description**:\n",
    "   - This notebook extracts and clusters seqlets based on the calculated attributions to identify motifs. It saves the clustering results.\n",
    "\n",
    "   **Input**:\n",
    "   - Attributions (`attributions.npy`)\n",
    "\n",
    "   **Output**:\n",
    "   - Positive clusters (`positive_clusters.npy`)\n",
    "   - Negative clusters (`negative_clusters.npy`)\n",
    "\n",
    "   **Checks**:\n",
    "   - Ensure that `positive_clusters.npy` and `negative_clusters.npy` are created in the `results/clusters/` directory.\n",
    "   - Verify the clusters to ensure they make sense.\n",
    "     ```python\n",
    "     # Load and check the clusters\n",
    "     positive_clusters = np.load('../results/clusters/positive_clusters.npy', allow_pickle=True).item()\n",
    "     negative_clusters = np.load('../results/clusters/negative_clusters.npy', allow_pickle=True).item()\n",
    "     \n",
    "     # Check content\n",
    "     print(positive_clusters.keys())  # Print the keys of positive clusters\n",
    "     print(negative_clusters.keys())  # Print the keys of negative clusters\n",
    "     ```\n",
    "\n",
    "### Quality Check for the Model\n",
    "\n",
    "To determine if the trained BPNet model is suitable for motif finding:\n",
    "\n",
    "1. **Training and Validation Loss**:\n",
    "   - Plot the training and validation loss to check for convergence.\n",
    "   - Look for signs of overfitting (e.g., training loss decreases but validation loss increases).\n",
    "\n",
    "2. **Accuracy**:\n",
    "   - Check the training and validation accuracy (if applicable) to ensure the model is learning the correct patterns.\n",
    "\n",
    "3. **Attributions**:\n",
    "   - Inspect the attributions to ensure they highlight meaningful parts of the sequences.\n",
    "   - High attributions should correspond to known motifs or significant regions in the sequences.\n",
    "\n",
    "4. **Cluster Analysis**:\n",
    "   - Inspect the clusters to ensure they group similar seqlets together.\n",
    "   - Visualize some of the motifs to see if they make biological sense.\n",
    "\n",
    "By performing these checks and analyses, you can ensure that each step of the workflow is correctly executed and that the final motifs identified by the model are meaningful and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56f9a91-c363-4d7b-a8c1-9044a039d31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57369,)\n",
      "(57369,)\n",
      "TGTCAGTATTACTATGTATGGTTTCTGAAAATACCAAATAAAAGATAATGAGGAATCTGAAAATATTTAaaaagacaccatggctttggcgttgcttgctctctcagatcactccctctgggaagcaagctgccatgtcatagagggacgcccacatggtgaggagctgaggcctccagccaatagccggcaaggaacaagagccttctgttgccagccctgggagtgagcttgcagacagatccttcagcctcaggaaagcttcttgatgacgcggtcctggctgacaaccggactgcaacctcatgagataccctgagccagaaccaccagctaagccacttctcaagtcctgactcaca\n",
      "5.60204\n"
     ]
    }
   ],
   "source": [
    "# Load and check the processed data\n",
    "X = np.load('../data/processed/X.npy')\n",
    "y = np.load('../data/processed/y.npy')\n",
    "\n",
    "# Check shapes\n",
    "print(X.shape)  # Expected shape: (number of peaks, sequence length)\n",
    "print(y.shape)  # Expected shape: (number of peaks,)\n",
    "\n",
    "# Check content\n",
    "print(X[0])  # Print the first sequence\n",
    "print(y[0])  # Print the first signal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925637a-880b-456b-9ffb-f7a88e2761f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
